{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import colorsys\n",
    "import time\n",
    "from time import time as timer\n",
    "\n",
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# Functions #\n",
    "#############\n",
    "\n",
    "def random_colors(N, bright=True):\n",
    "    \"\"\"\n",
    "    Generate random colors.\n",
    "    To get visually distinct colors, generate them in HSV space then\n",
    "    convert to RGB.\n",
    "    \"\"\"\n",
    "    brightness = 1.0 if bright else 0.7\n",
    "    hsv = [(i / N, 1, brightness) for i in range(N)]\n",
    "    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n",
    "    np.random.shuffle(colors)\n",
    "    return colors\n",
    "\n",
    "def random_colors(N):\n",
    "    np.random.seed(1)\n",
    "    colors = [tuple(255 * np.random.rand(3)) for _ in range(N)]\n",
    "    return colors\n",
    "\n",
    "def apply_mask(image, mask, color, alpha=0.5):\n",
    "    \"\"\"apply mask to image\"\"\"\n",
    "    for n, c in enumerate(color):\n",
    "        image[:, :, n] = np.where(\n",
    "            mask == 1,\n",
    "            image[:, :, n] * (1 - alpha) + alpha * c,\n",
    "            image[:, :, n]\n",
    "        )\n",
    "    return image\n",
    "\n",
    "def display_instances(image, boxes, masks, ids, names, scores):\n",
    "    \"\"\"\n",
    "        take the image and results and apply the mask, box, and Label\n",
    "    \"\"\"\n",
    "    n_instances = boxes.shape[0]\n",
    "    colors = random_colors(n_instances)\n",
    "\n",
    "    if not n_instances:\n",
    "        print('NO INSTANCES TO DISPLAY')\n",
    "    else:\n",
    "        assert boxes.shape[0] == masks.shape[-1] == ids.shape[0]\n",
    "\n",
    "    for i, color in enumerate(colors):\n",
    "        '''\n",
    "        Detect Only Person\n",
    "        '''\n",
    "        '''        \n",
    "        if ids[i] != 1:\n",
    "            continue\n",
    "        '''\n",
    "        '''\n",
    "        Detect Only Person\n",
    "        '''        \n",
    "        if not np.any(boxes[i]):\n",
    "            continue\n",
    "\n",
    "        y1, x1, y2, x2 = boxes[i]\n",
    "        label = names[ids[i]]\n",
    "        score = scores[i] if scores is not None else None\n",
    "        caption = '{} {:.2f}'.format(label, score) if score else label\n",
    "        mask = masks[:, :, i]\n",
    "\n",
    "        image = apply_mask(image, mask, color)\n",
    "        #image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "        image = cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255,0), 5)\n",
    "        #image = cv2.putText(image, caption, (x1, y1), cv2.FONT_HERSHEY_COMPLEX, 0.7, color, 2)\n",
    "        image = cv2.putText(image, caption, (x1, y1), cv2.FONT_HERSHEY_COMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cfchen/anaconda3/envs/MaskRCNN/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Path to Shapes trained weights\n",
    "#SHAPES_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_shapes.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one of the code blocks\n",
    "# MS COCO Dataset\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # To find local version\n",
    "import coco\n",
    "config = coco.CocoConfig()\n",
    "\n",
    "COCO_DIR = \"/media/cfchen/956df7bc-562e-4f24-8339-fd0b67f98888/Downloaded/COCO2017/2014\"  # TODO: enter value here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                93\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           coco\n",
      "NUM_CLASSES                    81\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Override the training configurations with a few\n",
    "# changes for inferencing.\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "# Inspect the model in training or inference modes\n",
    "# values: 'inference' or 'training'\n",
    "# TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=3.46s)\n",
      "creating index...\n",
      "index created!\n",
      "Images: 40137\n",
      "Classes: ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "# Build validation dataset\n",
    "if config.NAME == 'shapes':\n",
    "    dataset = shapes.ShapesDataset()\n",
    "    dataset.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "elif config.NAME == \"coco\":\n",
    "    dataset = coco.CocoDataset()\n",
    "    #dataset.load_coco(COCO_DIR, \"minival\")\n",
    "    dataset.load_coco(COCO_DIR, \"val\")\n",
    "\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_480p():\n",
    "    capture.set(3, 640)\n",
    "    capture.set(4, 360)\n",
    "    \n",
    "def change_res(width, height):\n",
    "    capture.set(3, width)\n",
    "    capture.set(4, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'capture' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-50345dabb5cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_480p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mchange_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m360\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-784874610afb>\u001b[0m in \u001b[0;36mmake_480p\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_480p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcapture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcapture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m360\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchange_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'capture' is not defined"
     ]
    }
   ],
   "source": [
    "make_480p()\n",
    "change_res(640, 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_frame(frame, percent=75):\n",
    "    width  = int(frame.shape[1] * percent/ 100)\n",
    "    height = int(frame.shape[0] * percent/ 100)\n",
    "    dim = (width, height)\n",
    "    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capture = cv2.VideoCapture(0)\n",
    "capture = cv2.VideoCapture('/media/cfchen/956df7bc-562e-4f24-8339-fd0b67f98888/Downloaded/VideosHPB/IMAG0011.mp4')\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi',fourcc, 30.0, (1920,1080))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Dim: 1920.0  X  1080.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Video Dim: {}  {}  {}\".format(capture.get(3), 'X', capture.get(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "Video Dim:  (360, 640, 3)\n",
      "++++++++ Video End ++++++++\n",
      "\n",
      "Elapsed time = 17.10716462135315 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "start = timer()\n",
    "\n",
    "count = 0;\n",
    "while(True):\n",
    "    ret, frame = capture.read() \n",
    "    \n",
    "    if ret == True:\n",
    "        frame = cv2.resize(frame, (640, 360)) \n",
    "        \n",
    "        #frame = rescale_frame(frame, percent=50)\n",
    "        \n",
    "        print(\"Video Dim:  {}\".format(frame.shape))\n",
    "\n",
    "        results = model.detect([frame], verbose=0)\n",
    "        #\n",
    "    \n",
    "        r = results[0]\n",
    "    \n",
    "        frame = display_instances(frame, r['rois'], r['masks'], r['class_ids'], dataset.class_names, r['scores'])\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        cv2.imshow('frame', frame)\n",
    "\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')) or (ret == False):\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "print('++++++++ Video End ++++++++')\n",
    "  \n",
    "capture.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print('\\nElapsed time = ' + str(timer() - start) + ' s\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++ Video End ++++++++\n",
      "\n",
      "Elapsed time = 75.03091025352478 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "        count = count + 1\n",
    "\n",
    "print(\"Video Dim:  {}\".format(frame.shape))\n",
    "frame = rescale_frame(frame, percent=50)\n",
    "frame = cv2.resize(frame, (640, 360)) \n",
    "resize= cv2.resize(frame, (640, 360)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++ Video End ++++++++\n",
      "\n",
      "Elapsed time = 0.0014584064483642578 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "start = timer()\n",
    "\n",
    "count = 0;\n",
    "while(True):\n",
    "    ret, frame = capture.read() \n",
    "\n",
    "    if ret == False:\n",
    "        break;\n",
    "    else:\n",
    "        count = count + 1\n",
    "    \n",
    "    #\n",
    "    '''\n",
    "    height , width , layers =  frame.shape\n",
    "    height = float(height)\n",
    "    width = float(width)\n",
    "    print(type(height))\n",
    "    new_h = height / 2\n",
    "    new_w = width / 2\n",
    "    frame = cv2.resize(frame, (new_w, new_h)) \n",
    "    '''\n",
    "    \n",
    "    frame = cv2.resize(frame, (640, 360)) \n",
    "    #\n",
    "    results = model.detect([frame], verbose=0)\n",
    "    #\n",
    "    r = results[0]\n",
    "    \n",
    "    frame = display_instances(frame, r['rois'], r['masks'], r['class_ids'], dataset.class_names, r['scores'])\n",
    "      \n",
    "        \n",
    "    out.write(frame)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if (cv2.waitKey(1) & 0xFF == ord('q')) or (ret == False):\n",
    "        break\n",
    "\n",
    "print('++++++++ Video End ++++++++')\n",
    "  \n",
    "capture.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print('\\nElapsed time = ' + str(timer() - start) + ' s\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
